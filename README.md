# ğŸª™ Competitor Data Pipeline

An automated pipeline to scrape competitor product data from [Kay.com](https://www.kay.com), normalize it, generate embeddings, and upload everything to **Elasticsearch** for search and analysis.

---

## ğŸ“‚ Project Structure
Pipeline_testing/
â”‚â”€â”€ details_from_urls.py # Scrape product details from URLs
â”‚â”€â”€ normalize_dataset.py # Clean + normalize dataset
â”‚â”€â”€ prepare_embeddings.py # Generate embeddings + upload to ES
â”‚â”€â”€ run_pipeline.py # End-to-end pipeline runner (CLI)
â”‚â”€â”€ urls_scraping.py # Scrape product URLs by category


---

## âš™ï¸ Requirements

### Install Python Packages
Create a virtual environment and install dependencies:

```bash
python3 -m venv .venv
source .venv/bin/activate    # Mac/Linux
.venv\Scripts\activate       # Windows

pip install -r requirements.txt

#---------------------------------

ğŸš€ Run Elasticsearch

Before running the pipeline, Elasticsearch must be running locally.

Download Elasticsearch.

Extract and go to the Elasticsearch folder

Start Elasticsearch:

bin/elasticsearch

By default it runs at:
ğŸ“ http://localhost:9200

âš ï¸ Youâ€™ll need the password for the elastic user (set during setup).


#---------------------------------

â–¶ï¸ Run the Pipeline

The entire pipeline can be run with one command:

python3 run_pipeline.py --password "your_es_password"    ##runs entire pipeline with 200 prodcuts per categpry ir 1200+products

Arguments

--limit â†’ Number of products per category to scrape (default = 200 for full run, set lower for testing).

--index â†’ Elasticsearch index name (e.g., competitor_offers or competitor_offers_test).

--password â†’ Elasticsearch elastic user password.

--start-from â†’ (optional) Choose which step to start from:

urls (default) â†’ Scrape URLs â†’ details â†’ normalize â†’ embeddings

details â†’ Start from product details scraping

normalize â†’ Start from normalization step

embeddings â†’ Only embeddings + upload

#---------------------------------

ğŸ“‚ Outputs

products_list.csv â†’ product URLs from scraping

poc_kay_final.csv â†’ raw scraped product details

poc_kay_normalized.csv â†’ normalized dataset

<index>.ndjson â†’ embeddings uploaded to Elasticsearch